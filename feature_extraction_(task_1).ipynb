{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_extraction (task 1).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOz3IU8jUsqe/t/1M5CcatY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-marshalova/methods-and-algorithms-of-computational-linguistics/blob/main/feature_extraction_(task_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAJlmfsTK2b_"
      },
      "source": [
        "Алгоритм выделения признаков текстов для классификации тональности твита на данных соревнования SentiRuEval-2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "800avbZsTB5h"
      },
      "source": [
        "from lxml import etree\n",
        "import csv\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkLo28uwTH8s"
      },
      "source": [
        "from typing import List, Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmIBAOCOv478",
        "outputId": "9afbd0ea-8624-411b-8204-5c560496a5bd"
      },
      "source": [
        "pip install spacy_udpipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy_udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 40.7 MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 29.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.62.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (57.4.0)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.19.5)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.1)\n",
            "Collecting thinc<8.1.0,>=8.0.9\n",
            "  Downloading thinc-8.0.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (623 kB)\n",
            "\u001b[K     |████████████████████████████████| 623 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.23.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->spacy_udpipe) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.1)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626610 sha256=675e2332c335e7c41e1b45915298078f57d599633e4182f97522e175c532ca4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, ufal.udpipe, spacy, spacy-udpipe\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.0 pydantic-1.8.2 spacy-3.1.3 spacy-legacy-3.0.8 spacy-udpipe-1.0.0 srsly-2.4.1 thinc-8.0.10 typer-0.4.0 ufal.udpipe-1.2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUfuT7QCrpFb",
        "outputId": "b236db93-0ce9-4a2c-efaf-61d000261f93"
      },
      "source": [
        "import spacy_udpipe\n",
        "\n",
        "spacy_udpipe.download(\"ru\") \n",
        "nlp = spacy_udpipe.load(\"ru\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded pre-trained UDPipe model for 'ru' language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8SAP0EXRTMG"
      },
      "source": [
        "Загружаем словарь оценочных слов и выражений русского языка [РуСентиЛекс](http://www.labinform.ru/pub/rusentilex/index.htm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxQd2nO6Hnfz"
      },
      "source": [
        "def load_emotions(filename):\n",
        "  with open(\"emo_dict.csv\", encoding=\"utf-8\") as file:\n",
        "    all_words = csv.DictReader(file)\n",
        "    positive_list=[]\n",
        "    negative_list=[]\n",
        "    neutral_list=[]\n",
        "    for row in all_words:\n",
        "                row = list(row.values())[0].split(';')\n",
        "                if 'PSTV' in row:\n",
        "                    positive_list.append(row[0])\n",
        "                elif 'NGTV' in row:\n",
        "                    negative_list.append(row[0])\n",
        "                elif 'NEUT' in row:\n",
        "                    neutral_list.append(row[0])\n",
        "  return positive_list,negative_list,neutral_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1Y0UMBNNxyr"
      },
      "source": [
        "positive_list,negative_list,neutral_list=load_emotions('emo_dict.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTS37G7TSKJM"
      },
      "source": [
        "Загружаем [список](https://habr.com/ru/post/21949/) стран, городов и регионов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjmhg2hsKRLr"
      },
      "source": [
        " def load_places(file_name):\n",
        "  places=[]\n",
        "  with open(file_name, mode='rb') as fp:\n",
        "        xml_data = fp.read()\n",
        "        root = etree.fromstring(xml_data)\n",
        "        for place in root.getchildren():\n",
        "              for row in place.getchildren():\n",
        "                if row.tag=='name':\n",
        "                    places.append(row.text.lower())\n",
        "  return places"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gch9uZ1aN36l"
      },
      "source": [
        "places=load_places('rocid.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsUR4c8lN6kc"
      },
      "source": [
        "extract=[('telegram',re.compile('(http[s]?:)?\\/\\/t\\.co')),('hashtag',re.compile('#.?'))]\n",
        "delete=[('number',re.compile('[+-]?[0-9\\-\\.,]+[%]?')), ('url', re.compile('http')),('mention',re.compile('@.+')),('e-mail', re.compile('.+@.+\\.')),('hashtag',re.compile('#.?'))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EercFs4Vshfj"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2OeBUY5r17h"
      },
      "source": [
        "class MyVectorizer(BaseEstimator,TransformerMixin):\n",
        "  def __init__(self,texts,vectors):\n",
        "    self.texts=texts\n",
        "    self.vectors=vectors\n",
        "    self.features=['telegram','hashtag','geo','positive','negative','neutral','exclamation']\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, X, y=None):\n",
        "    return self.vectors\n",
        "  def get_feature_names(self):\n",
        "    return self.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aide6G9H1fU8"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSKqYWpP1mnu"
      },
      "source": [
        "vectorizer = CountVectorizer(lowercase=True, min_df=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w8XfYn_2mUB"
      },
      "source": [
        "from sklearn.feature_selection import SelectPercentile, chi2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z645I9w_2ngD"
      },
      "source": [
        "selector = SelectPercentile(chi2, percentile=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEuroGo03YWy"
      },
      "source": [
        "from sklearn.pipeline import FeatureUnion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znoGcfsay8Sz"
      },
      "source": [
        "class FeatureExtractor:\n",
        "  def __init__(self,file):\n",
        "    self.texts, self.vectors,self.labels=self.load_sentirueval_2016(file)\n",
        "    print('loaded successfully')\n",
        "    vectorizer.fit(self.texts)\n",
        "    self.X=vectorizer.transform(self.texts)\n",
        "    print('transformed successfully')\n",
        "    tfidf=TfidfTransformer()\n",
        "    myvec=MyVectorizer(self.texts,self.vectors)\n",
        "    transformer =FeatureUnion([('tfidf',tfidf),('myvec',myvec)])\n",
        "    transformer.fit(self.X)\n",
        "    self.X_transformed = transformer.transform(self.X)\n",
        "    print('binary features added successfully')\n",
        "    # как я поняла, idf нужны для сортировки, поэтому добавленным признакам можно сопоставить любое число. я выбрала 10, чтобы эти признаки оказались в начале списка\n",
        "    numbers=[10 for i in enumerate(myvec.get_feature_names())]\n",
        "    idfs=[idf for idf in tfidf.idf_]+numbers\n",
        "    self.tokens_with_IDF = list(zip(vectorizer.get_feature_names()+myvec.get_feature_names(), idfs))\n",
        "    selector.fit(self.X_transformed, self.labels)\n",
        "    self.selected_tokens_with_IDF = [self.tokens_with_IDF[idx] for idx in selector.get_support(indices=True)]\n",
        "    print('selected successfully')\n",
        "    self.sorted_selected_tokens_with_IDF=sorted(self.selected_tokens_with_IDF, key=lambda it: (-it[1], it[0]))\n",
        "    print('sorted successfully')\n",
        "\n",
        "  def load_sentirueval_2016(self,file_name: str) -> Tuple[List[str], List[str]]:\n",
        "    texts = []\n",
        "    labels = []\n",
        "    vectors=[]\n",
        "    with open(file_name, mode='rb') as fp:\n",
        "        xml_data = fp.read()\n",
        "    root = etree.fromstring(xml_data)\n",
        "    for database in root.getchildren():\n",
        "        if database.tag == 'database':\n",
        "            for table in database.getchildren():\n",
        "                if table.tag != 'table':\n",
        "                    continue\n",
        "                new_text = None\n",
        "                new_label = None\n",
        "                for column in table.getchildren():\n",
        "                    if column.get('name') == 'text':\n",
        "                        #лемматизация\n",
        "                        new_text,vector = self.lemmatize(str(column.text).strip())\n",
        "                        if new_label is not None:\n",
        "                            break\n",
        "                    elif column.get('name') not in {'id', 'twitid', 'date'}:\n",
        "                        if new_label is None:\n",
        "                            label_candidate = str(column.text).strip()\n",
        "                            if label_candidate in {'0', '1', '-1'}:\n",
        "                                new_label = 'negative' if label_candidate == '-1' else \\\n",
        "                                    ('positive' if label_candidate == '1' else 'neutral')\n",
        "                                if new_text is not None:\n",
        "                                    break\n",
        "                if (new_text is None) or (new_label is None):\n",
        "                    raise ValueError('File `{0}` contains some error!'.format(file_name))\n",
        "                texts.append(new_text)\n",
        "                vectors.append(vector)\n",
        "                labels.append(new_label)\n",
        "            break\n",
        "    return texts, vectors,labels\n",
        "\n",
        "  def lemmatize(self,text):\n",
        "   text_lemmatized=[]\n",
        "   vector=[]\n",
        "   geo=0\n",
        "   positive=0\n",
        "   negative=0\n",
        "   neutral=0\n",
        "   excl=0\n",
        "   doc = nlp(text.lower())\n",
        "   #добавляем бинарные признаки наличия ссылки на телеграм-канал и хештега\n",
        "   for element in extract:\n",
        "       if element[1].search(text):\n",
        "         vector.append(1)\n",
        "       else:\n",
        "         vector.append(0)\n",
        "   for token in doc:\n",
        "     lemma=token.lemma_\n",
        "     #удаляем геграфические названия и записываем их наличие\n",
        "     if lemma in places:\n",
        "       geo=1\n",
        "       lemma=''\n",
        "    #добавляем бинарные признаки наличия эмоциональных слов\n",
        "     if lemma in positive_list:\n",
        "      positive=1\n",
        "     if lemma in negative_list:\n",
        "      negative=1\n",
        "     if lemma in neutral_list:\n",
        "      neutral=1\n",
        "    #добавляем бинарный признак наличия восклицательного знака\n",
        "     if lemma == '!':\n",
        "       excl=1\n",
        "     #удаляем лишнее\n",
        "     for element in delete:\n",
        "       if element[1].match(lemma):\n",
        "         lemma=''\n",
        "     #удаляем элементы, для которых часть речи не определилась\n",
        "     if token.pos_=='X':\n",
        "       lemma=''\n",
        "     text_lemmatized.append(lemma)\n",
        "   vector.extend([geo,positive,negative,neutral,excl])\n",
        "   return  ' '.join(text_lemmatized),vector\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ukDdOvFPArk"
      },
      "source": [
        "def print_features(features):\n",
        "    for feature, idf in features [0:40]: print('{0:.6f} => {1}'.format(idf, feature))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jj0OXnPPrPR",
        "outputId": "128fc849-9ddf-47f8-e9f5-e6e9b3ad8f18"
      },
      "source": [
        "banks=FeatureExtractor('bank_train_2016.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded successfully\n",
            "transformed successfully\n",
            "binary features added successfully\n",
            "selected successfully\n",
            "sorted successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG9Ic3-cPrhd",
        "outputId": "7fa2aa33-15b1-41c5-b099-30dab57e0516"
      },
      "source": [
        "print_features(banks.sorted_selected_tokens_with_IDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.000000 => exclamation\n",
            "10.000000 => hashtag\n",
            "10.000000 => positive\n",
            "10.000000 => telegram\n",
            "8.761426 => ауэрбанка\n",
            "8.761426 => бум\n",
            "8.761426 => бурятие\n",
            "8.761426 => внедрение\n",
            "8.761426 => вносить\n",
            "8.761426 => выбирать\n",
            "8.761426 => говно\n",
            "8.761426 => догонять\n",
            "8.761426 => компание\n",
            "8.761426 => любимый\n",
            "8.761426 => миллионер\n",
            "8.761426 => навстретить\n",
            "8.761426 => навязывать\n",
            "8.761426 => неустойка\n",
            "8.761426 => ок\n",
            "8.761426 => олы\n",
            "8.761426 => оперативно\n",
            "8.761426 => популярность\n",
            "8.761426 => стабильно\n",
            "8.761426 => супер\n",
            "8.761426 => тысячный\n",
            "8.761426 => убить\n",
            "8.761426 => уэк\n",
            "8.761426 => фон\n",
            "8.761426 => часы\n",
            "8.761426 => школьный\n",
            "8.761426 => юникорбанка\n",
            "8.538282 => бренд\n",
            "8.538282 => введёть\n",
            "8.538282 => воровство\n",
            "8.538282 => впервые\n",
            "8.538282 => гореть\n",
            "8.538282 => горь\n",
            "8.538282 => заморозить\n",
            "8.538282 => идиот\n",
            "8.538282 => красивый\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1tfW9fMP7f6",
        "outputId": "bfdea5bc-2a80-4644-b5fa-a78652600795"
      },
      "source": [
        "tkks=FeatureExtractor('tkk_train_2016.xml')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded successfully\n",
            "transformed successfully\n",
            "binary features added successfully\n",
            "selected successfully\n",
            "sorted successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hQKtZbrP7f8",
        "outputId": "f1837d98-8ac9-43d5-9854-526cd4b7b17a"
      },
      "source": [
        "print_features(tkks.sorted_selected_tokens_with_IDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.000000 => exclamation\n",
            "10.000000 => hashtag\n",
            "10.000000 => positive\n",
            "10.000000 => telegram\n",
            "8.678326 => виваселла\n",
            "8.678326 => внедрять\n",
            "8.678326 => вэб\n",
            "8.678326 => дзержинска\n",
            "8.678326 => долой\n",
            "8.678326 => емкость\n",
            "8.678326 => заботиться\n",
            "8.678326 => защищать\n",
            "8.678326 => инноватор\n",
            "8.678326 => магазине\n",
            "8.678326 => мичуринска\n",
            "8.678326 => настроить\n",
            "8.678326 => оборудовать\n",
            "8.678326 => перерыв\n",
            "8.678326 => поболтать\n",
            "8.678326 => подарочка\n",
            "8.678326 => поезд\n",
            "8.678326 => развернуть\n",
            "8.678326 => сверхскоростный\n",
            "8.678326 => совершать\n",
            "8.678326 => сёдня\n",
            "8.678326 => устраивать\n",
            "8.678326 => цепь\n",
            "8.678326 => шикарный\n",
            "8.455183 => ddos\n",
            "8.455183 => акселерационный\n",
            "8.455183 => атаковать\n",
            "8.455183 => бля\n",
            "8.455183 => вгтрк\n",
            "8.455183 => видеоконференция\n",
            "8.455183 => волс\n",
            "8.455183 => выручить\n",
            "8.455183 => гребанный\n",
            "8.455183 => доход\n",
            "8.455183 => европейский\n",
            "8.455183 => задействовать\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7rOV8KjmF8l"
      },
      "source": [
        "Таким образом, наличие географических названий, нейтральных и негативных слов оказались незначимыми признаками. Возможно, это потому, что геграфические названия могут встретиться в любых твитах (а не только нейтральных, как я предпологала), а подгружаемый словарь эмоциональных слов основан на ассоциациях. Допустим, слово \"аборт\" в нем отмечено как негативное, потому что вызывает негативные ассоциации, но встретится оно, скорее, в нейтральном твите. \n",
        "\n",
        "А наличие положительных прилагательных, ссылки на телеграм канал, хештега и восклицательного знака в обоих случаях значимы. Вероятно, телеграм-ссылка - признак нейтральных твитов, а хештег и восклицательный знак чаще встречаются в эмоциональных твитах (причем, скорее всего, положительных). \n",
        "\n",
        "Еще, возможно, некоторые слова оказались значимыми признаками из-за странной лемматизации. Например, \"льзуются\" - это, скорее всего, лемматизированная часть слова \"пользуются\", которое в твите, вероятно, было написано раздельно.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}